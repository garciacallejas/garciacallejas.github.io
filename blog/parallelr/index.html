<!DOCTYPE html>
<html lang="en-us">
<head prefix="og: http://ogp.me/ns#">
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
  <meta property="og:title" content=" Parallel R with SLURM &middot;  David García Callejas" />
  
  <meta property="og:site_name" content="David García Callejas" />
  <meta property="og:url" content="/blog/parallelr/" />
  
  
  <meta property="og:type" content="article" />
  
  <meta property="og:article:published_time" content="2020-05-18T00:00:00Z" />
  
  

  <title>
     Parallel R with SLURM &middot;  David García Callejas
  </title>

  <link rel="stylesheet" href="/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/css/main.css" />
  <link rel="stylesheet" href="/css/font-awesome.min.css" />
  <link rel="stylesheet" href="/css/github.css" />
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400" type="text/css">
  <link rel="shortcut icon" href="/images/favicon.ico" />
  <link rel="apple-touch-icon" href="/images/apple-touch-icon.png" />
  
</head>
<body>
    <header class="global-header"  style="background-image:url( /images/bg.jpg )">
    <section class="header-text">
      <h1><a href="/">David García Callejas</a></h1>
      
      <div class="sns-links hidden-print">
  
  <a href="mailto:david.garcia.callejas@gmail.com">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/@callejas_eco" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  <a href="https://scholar.google.com/citations?user=Tx-hZQEAAAAJ%26hl%3den" target="_blank">
    <i class="fa fa-google"></i>
  </a>
  
  
  
  <a href="https://github.com/garciacallejas" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  
  
</div>
      
    </section>
  </header>
  <main class="container">


<article>
  <header>
    <h1 class="text-primary">Parallel R with SLURM</h1>
    <div class="post-meta clearfix">
    
      <div class="post-date pull-left">
        Posted on
        <time datetime="2020-05-18T00:00:00Z">
          May 18, 2020
        </time>
      </div>
    
      <div class="pull-right">
        
      </div>
    </div>
  </header>
  <section>
    


<p><em>This post is a work in progress! I will incorporate new ideas as I try them</em></p>
<div id="types-of-parallelization" class="section level2">
<h2>Types of parallelization</h2>
<p>SLURM is a workload manager for organizing computing resources in Linux clusters. In order to submit jobs to a cluster managed via SLURM, these have to be submitted via bash scripts that call the actual program to run. Focusing on R, one may think on two general types of parallelizing scripts. First, ‘internal’ parallelization within your script can be accomplished using the <code>foreach</code> and <code>doParallel</code> packages. Second, ‘external’ parallelization setting up a job array in SLURM.</p>
</div>
<div id="parallelization-with-foreach" class="section level2">
<h2>Parallelization with foreach</h2>
<p>The key here is to generate a number of jobs, assign them to actual cores, and split <code>foreach</code> loops among these jobs. A basic skeleton of such workflow, in Linux, is:</p>
<pre class="r"><code># load packages -----------------------------------------------------------
library(foreach)
library(doParallel)

# set number of cores -----------------------------------------------------
workers &lt;- 10
cl &lt;- parallel::makeCluster(workers)
# register the cluster for using foreach
doParallel::registerDoParallel(cl)

# run some time-intensive task --------------------------------------------
x &lt;- iris[which(iris[,5] != &quot;setosa&quot;), c(1,5)]
trials &lt;- 10000

r &lt;- foreach(icount(trials), 
             .combine=cbind) %dopar% {
               ind &lt;- sample(100, 100, replace=TRUE)
               result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))
               coefficients(result1)
             }</code></pre>
<p>The first interesting part is setting up the number of workers. Hardly-coding it is appropriate, obviously, when you know in advance how many jobs you want to distribute for your loop. In a single computer, this is all that is needed, but below I explore how to combine this with the options from SLURM to actually use as many CPUs as needed, distributed among different nodes of a cluster. But first, let’s look at the structure of <code>foreach</code>. Despite its name, which reminds of a <code>for</code> loop, <code>foreach</code> is better thought of as a parallelized version of <code>apply</code> functions. Note that, first of all, a <code>foreach</code> loop returns an object, unlike standard <code>for</code> loops. Thus, inside the loop there will be some calculations and, importantly, the returning object (in this example, <code>coefficients(result1)</code>). Each iteration of the loop generates an instance of the object returned, and an important point is that <code>foreach</code> combines the result from all these iterations through the <code>.combine</code> argument. By default, <code>foreach</code> loops return a list with as many elements as iterations. Aside from lists, one may want to append the results of each iteration as columns to a dataframe, as in this example, or as rows (<code>rbind</code>), but more complex options are also possible. For example, if you want to return two different dataframes, that are to be combined row-wise, you need to define a tailored combine function and specify that function in the
<code>foreach</code> call:</p>
<pre class="r"><code>comb.fun &lt;- function(...) {
  mapply(&#39;rbind&#39;, ..., SIMPLIFY=FALSE)
}

x &lt;- iris[which(iris[,5] != &quot;setosa&quot;), c(1,5)]
trials &lt;- 1000

r &lt;- foreach(icount(trials), 
             .combine=comb.fun) %dopar% {
               ind &lt;- sample(100, 100, replace=TRUE)
               result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))
               df1 &lt;- data.frame(intercept = coefficients(result1)[1], 
                                 slope = coefficients(result1)[2],
                                 row.names = NULL)
               df2 &lt;- data.frame(x = rnorm(1,0,1), y = runif(1,0,1))
               list(df1,df2)
             }

# the object returned is a list
head(r[[1]])</code></pre>
<pre><code>##    intercept    slope
## 1 -10.916152 1.714646
## 2 -11.957857 1.970020
## 3 -11.341890 1.814531
## 4 -11.356582 1.811111
## 5  -8.072459 1.326202
## 6 -12.778837 1.982608</code></pre>
<pre class="r"><code>head(r[[2]])</code></pre>
<pre><code>##            x          y
## 1 -2.6651276 0.29482016
## 2 -0.8154548 0.06786526
## 3  0.9657205 0.85058731
## 4  0.3407531 0.62811342
## 5 -0.6727373 0.65518517
## 6 -1.3815257 0.06919888</code></pre>
<p>Note that in output from each iteration is packed in a list, and the combine function binds by rows each element of the list. Lastly, note also that <code>%dopar%</code> is the command that parallelizes the loop. The same loop can be run in a sequential setting, replacing <code>%dopar</code> with <code>%do%</code>.</p>
<p>When using this setting in a cluster, you need to call SLURM with the appropriate options depending on your needs (basically, how many CPUs you want to use). The command for launching your program through SLURM is <code>srun</code>, and the SLURM script itself is called using the <code>sbatch</code> command in the shell. This is how a basic SLURM script looks like:</p>
<pre class="bash"><code>#!/bin/bash

#------- Descripción del trabajo -------

#SBATCH --job-name=&#39;TEST&#39;
#SBATCH --comment=&#39;SLURM test&#39;

#------- Avisos -------

#SBATCH --mail-user=[your email]
#SBATCH --mail-type=END,FAIL,TIME_LIMIT_80

#------- Parametrización -------

#SBATCH --requeue
#SBATCH --share

#SBATCH --nodes=10
#SBATCH --tasks-per-node=16
#SBATCH --cpus-per-task=1
#SBATCH --partition=cn
#SBATCH --mem=1G
#SBATCH --time=2-0:0:0

#------- Entrada/Salida -------

#SBATCH --workdir=[your working directory]
#SBATCH --output=[full path to output file]
#SBATCH --error=[full path to error log]

#------- Carga de módulos -------

module load R

export R_LIBS=/path.to/R/library

#------- Comando -------
srun Rscript --no-save TEST.R</code></pre>
<p>Here I included an option to use your local R library in case you want to use locally-installed packages, or if the server does not have a certain package installed. The output and error files will hold the messages and error outputs arising from your R script. These may be set to /dev/null, if you do not want to keep them. This script will allocate 10 * 16 * 1 CPUs, so in principle you could specify 160 workers in your R script and each would be allocated one CPU. This reasoning, however, is only part of the story. Apparently, in order to make R work with SLURM across different nodes, it is not enough to be consistent in the number of CPUs allocated.</p>
</div>
<div id="parallelization-through-slurm-job-arrays" class="section level2">
<h2>Parallelization through SLURM job arrays</h2>
</div>

  </section>
</article>

  </main>
  <footer class="container global-footer">
    <div class="copyright-note pull-left">
      
    </div>
    <div class="sns-links hidden-print">
  
  <a href="mailto:david.garcia.callejas@gmail.com">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/@callejas_eco" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  <a href="https://scholar.google.com/citations?user=Tx-hZQEAAAAJ%26hl%3den" target="_blank">
    <i class="fa fa-google"></i>
  </a>
  
  
  
  <a href="https://github.com/garciacallejas" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  
  
</div>

  </footer>

  <script src="/js/highlight.pack.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
  
  
</body>
</html>

